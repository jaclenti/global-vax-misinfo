{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 Cross Countries URLs - Save Data\n",
    "In this notebook we analyse the retweets containing URLs from one country to another.\n",
    "We extract them, flag the low-credible domains and study the misinformation flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"/data/public/jlenti/multilang-vax/DATA_clean_url\"\n",
    "\n",
    "#list of neutral domains (url shorteners, twitter.com, facebook.com)\n",
    "neutrals = pd.read_csv(\"/home/jlenti/Files/neutral_domains_1309.txt\")[\"0\"].tolist()\n",
    "#list of low-credible domains\n",
    "blacklist = pd.read_csv(\"/home/jlenti/Files/merged_blacklist_1309.txt\")[\"0\"].tolist()\n",
    "blacklist.extend(pd.read_csv( '/home/jlenti/Files/lemonde_blacklist_2709.txt')[\"domain\"].tolist())\n",
    "blacklist.extend(pd.read_csv('/home/jlenti/Files/greek_blacklist_1009.txt')[\"0\"].tolist())\n",
    "\n",
    "#domains associated to youtube\n",
    "youtube_domains = [\"youtube.com\", \"youtu.be\"]\n",
    "\n",
    "#list of all countries (size ordered)\n",
    "countries = [\"US\", \"BR\", \"AR\", \"GB\", \"ES\", \"MX\", \"FR\", \"CA\", \"TR\", \"VE\", \"AU\", \"CO\", \"IT\", \"CL\", \"DE\",\n",
    "             \"PT\", \"IE\", \"PY\", \"EC\", \"RU\", \"UY\", \"NZ\", \"PL\", \"NL\", \"PE\", \"CU\", \"PA\", \"GR\"]\n",
    "#countries speaking english or italian, the ones with a list of low-credible domains\n",
    "LC_countries = [\"IT\", \"US\", \"GB\", \"AU\", \"NZ\", \"IE\"]\n",
    "#sorted by language\n",
    "lang_sort = [\"US\", \"IE\", \"GB\", \"CA\", \"NZ\", \"AU\", \"FR\", \"IT\", \"PL\", \"NL\", \"DE\", \"RU\", \"TR\", \n",
    "             \"BR\", \"PT\", \"GR\", \"AR\", \"ES\", \"MX\",\"VE\", \"CO\", \"CL\",\n",
    "             \"PY\", \"EC\", \"UY\", \"PE\", \"CU\", \"PA\"]\n",
    "\n",
    "#named periods\n",
    "periods = {\"period1\": [\"201910\", \"201911\", \"201912\"],\n",
    "           \"period2\": [\"202007\", \"202008\", \"202009\"],\n",
    "           \"period3\": [\"202010\", \"202011\", \"202012\"],\n",
    "           \"period4\": [\"202101\", \"202102\", \"202103\"]\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of users with wrong geolocations\n",
    "filtered_users = pd.concat([pd.read_csv(\"/data/public/jlenti/multilang-vax/Geolocation_Mismatches/more_countries_users_RT.csv\"),\n",
    "                            pd.read_csv(\"/data/public/jlenti/multilang-vax/Geolocation_Mismatches/misgeo_popular_user_countries_pairs.csv\")])[\"user\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from \"/data/public/jlenti/multilang-vax/DATA_clean_url\" I query the tweets that are retweets between \n",
    "#different countries (in our list of countries), and that contain a url\n",
    "#I create a dictionary associating the period to the corresponding dataframe (with all languages)\n",
    "cross_border_urls = {p: pd.concat([pd.read_csv(file, sep = \"\\t\", lineterminator = \"\\n\",\n",
    "                                               low_memory = False, quoting = False,\n",
    "                                               escapechar = None) \\\n",
    "                                   [[\"user_screen_name\", \"RT_user_screen_name\", \"user_country_code\", \n",
    "                                     \"RT_user_country_code\", \"urls\", \"lang\"]] \\\n",
    "                                    #keep only retweets beween different countries of our list with a url\n",
    "                                   .query(\"urls == urls\") \\\n",
    "                                   .query(\"(user_country_code in @countries)&(RT_user_country_code in @countries)&(user_country_code != RT_user_country_code)\") \\\n",
    "                                   .query(\"(user_screen_name not in @filtered_users)&(RT_user_screen_name not in @filtered_users)\")\n",
    "                                   #each periods has 3 months (in dictionary periods)\n",
    "                                   for month in periods[p]\n",
    "                                   #files in the folder have the format folder/it/20200101-it.....tsv.gz\n",
    "                                   #to keep all data from a specific month I select folder/*/month*\n",
    "                                   for file in sorted(glob(\"/\".join([folder, \"*\", month + \"*\"])))]) \n",
    "                     for p in periods}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in periods:\n",
    "    #I use urlparse from library urllib to extract the domain from all the urls\n",
    "    #I remove the head www. from all domains\n",
    "    cross_border_urls[p][\"domain\"] = cross_border_urls[p][\"urls\"].apply(lambda x: urlparse(x).netloc).apply(lambda x: x[4:] if x[:4] == \"www.\" else x)\n",
    "    #label low-credible domains with LC True (that are stored in blacklist)\n",
    "    cross_border_urls[p][\"LC\"] = cross_border_urls[p][\"domain\"].isin(blacklist)\n",
    "    #label neutral domains with neutral True (that are stored in neutrals. urls shorteners, social networks, or generic domains)\n",
    "    cross_border_urls[p][\"neutral\"] = cross_border_urls[p][\"domain\"].isin(neutrals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate the dictionary of dataframes to a unique dataframe with column \"period\"\n",
    "cross_urls_df = pd.concat([cross_border_urls[p].assign(period = p) for p in periods])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross_urls_df.to_csv(\"/home/jlenti/Files/cross_border_retweeted_urls_2104.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
